# od-parse

> **ðŸ¤– LLM-First Document Parser:** od-parse now requires LLM API keys for advanced document understanding. See [Installation](#installation) for setup instructions.

An enterprise-grade, LLM-powered library for parsing complex PDFs using state-of-the-art language models. This library leverages the power of GPT-4, Claude, and Gemini for superior document understanding, including:

- **LLM-Powered Analysis**: GPT-4, Claude 3.5 Sonnet, Gemini 1.5 Pro for complex document understanding
- **Vision-Language Models**: Advanced multimodal AI for visual document analysis
- **Intelligent Document Classification**: 54+ document types with context-aware processing
- **Complex Layout Understanding**: Multi-column, tables, forms, handwriting recognition
- **Domain-Specific Processing**: Tax forms, legal contracts, medical records, financial statements
- **Structured Data Extraction**: JSON output with high accuracy and validation
- **Cost-Optimized Model Selection**: Automatic model selection based on document complexity

## Features

### Core Features (No Heavy Dependencies)
- **ðŸ¤– LLM-Powered Document Understanding**: GPT-4, Claude 3.5 Sonnet, Gemini 2.0 for complex document analysis
- **ðŸ“„ PDF Parsing**: Text, tables, forms, and image extraction from PDFs
- **ðŸ“Š Excel Processing**: DuckDB-powered Excel parsing with SQL queries, JSON/Markdown output
- **ðŸŽ¯ Smart Document Classification**: Automatically identify 54+ document types with intelligent routing
- **ðŸ—ï¸ Structured Data Output**: High-accuracy JSON extraction with validation and confidence scoring
- **ðŸ’° Cost-Optimized Processing**: Automatic model selection based on document complexity and cost
- **ðŸ”„ Intelligent Fallbacks**: Graceful degradation when LLM services are unavailable
- **ðŸŒ Multi-Provider Support**: OpenAI, Anthropic, Google, Azure OpenAI, and local vLLM models
- **ðŸ“ˆ Enterprise-Ready**: Batch processing, API integrations, and scalable architecture
- **ðŸ”’ Privacy-Conscious**: Support for local models and on-premises deployment

### Advanced AI Features (Optional - Requires PyTorch)
- **ðŸ‘ï¸ TrOCR**: Transformer-based OCR for superior text recognition
- **ðŸ“Š Table Transformer**: Neural network-based table detection and extraction
- **ðŸ§  LLaVA-NeXT**: Vision-Language AI for document understanding

## Installation

> **ðŸ¤– LLM Required:** od-parse now requires LLM API keys for document processing. Set up your API keys before installation.

> **ðŸ“¦ Installation:** od-parse is distributed via GitHub. Install with `pip install git+https://github.com/octondata/od-parse.git` or download wheel files from [GitHub Releases](https://github.com/octondata/od-parse/releases).

### Step 1: Set Up LLM API Keys

Choose one or more LLM providers and set up your API keys:

```bash
# OpenAI (Recommended for best performance)
export OPENAI_API_KEY="your-openai-api-key"

# Anthropic Claude (Excellent for complex documents)
export ANTHROPIC_API_KEY="your-anthropic-api-key"

# Google Gemini (Great for large documents)
export GOOGLE_API_KEY="your-google-api-key"

# Azure OpenAI (Enterprise option)
export AZURE_OPENAI_API_KEY="your-azure-key"
export AZURE_OPENAI_ENDPOINT="your-azure-endpoint"
```

### Step 2: Installation

**Option 1: Install from GitHub (Recommended)** â­
```bash
pip install git+https://github.com/octondata/od-parse.git
```

**Option 2: Install from Wheel File (From GitHub Releases)**
```bash
# Download wheel from: https://github.com/octondata/od-parse/releases
pip install od_parse-0.2.0-py3-none-any.whl
```

**Option 3: Install from Source (For Development)**

```bash
# Clone the repository
git clone https://github.com/octondata/od-parse.git
cd od-parse

# Create a virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install in development mode
pip install -e .

# Or install with advanced features
pip install -e .[advanced]
pip install -e .[all]
```

> **ðŸ’¡ Note:** See [docs/INSTALLATION_METHODS.md](docs/INSTALLATION_METHODS.md) for all installation methods and best practices.

### Quick Setup Script

For convenience, you can use the automated setup script:

```bash
# Clone and run setup script
git clone https://github.com/octondata/od-parse.git
cd od-parse
chmod +x setup_dev.sh
./setup_dev.sh
```

This script will:
- âœ… Create a virtual environment
- âœ… Install all dependencies
- âœ… Install od-parse in development mode
- âœ… Test the installation
- âœ… Provide next steps

### Alternative: Direct Download

If you don't have git, you can download and install directly:

```bash
# Download the source code
wget https://github.com/octondata/od-parse/archive/main.zip
unzip main.zip
cd od-parse-main

# Create virtual environment and install
python3 -m venv venv
source venv/bin/activate
pip install -e .
```

### Future PyPI Installation (Coming Soon)

Once published to PyPI, you'll be able to install with:

```bash
# Install the basic package (COMING SOON)
pip install od-parse

# Install with all advanced features (COMING SOON)
pip install "od-parse[all]"

# Install with specific advanced features (COMING SOON)
pip install "od-parse[trocr]"              # TrOCR text recognition
pip install "od-parse[table_transformer]"  # Neural table extraction
pip install "od-parse[llava_next]"         # Document understanding with VLMs
pip install "od-parse[multilingual]"       # Multi-language support
pip install "od-parse[quality_assessment]" # Quality assessment metrics
pip install "od-parse[async_processing]"   # Async processing capabilities

# Install preset combinations (COMING SOON)
pip install "od-parse[basic]"        # Essential features
pip install "od-parse[advanced]"     # All stable features
pip install "od-parse[experimental]" # All features including experimental
```

### Installation Troubleshooting

**âŒ Error: "No LLM API keys found"**

This is the most common error. od-parse requires LLM access:

```bash
# Set at least one API key
export OPENAI_API_KEY="your-key-here"
# OR
export ANTHROPIC_API_KEY="your-key-here"
# OR
export GOOGLE_API_KEY="your-key-here"
```

**âŒ Error: "Could not find a version that satisfies the requirement od-parse"**

This error occurs because `od-parse` is not yet published to PyPI. Use the development installation method above.

**âŒ Error: "externally-managed-environment"**

This is a safety feature in newer Python versions. Always use a virtual environment:

```bash
# Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Then install
pip install -e .
```

**âŒ Error: "pip: command not found"**

Use `python3 -m pip` instead:

```bash
python3 -m pip install -e .
```

**âŒ Missing Dependencies**

Install core dependencies manually if needed:

```bash
pip install pdfminer.six tabula-py opencv-python pillow pytesseract pandas numpy openai anthropic google-generativeai pdf2image
```

> **ðŸ¤– LLM-First Approach:** od-parse now requires LLM API keys for advanced document understanding. Local processing is available for basic extraction only.

## Quick Start

```python
import os
from od_parse import parse_pdf, convert_to_markdown

# Set your LLM API key (never commit actual keys!)
os.environ["OPENAI_API_KEY"] = "your-api-key-here"

# Parse a PDF with LLM-powered understanding
result = parse_pdf("path/to/document.pdf")

# Access LLM analysis results
llm_analysis = result['parsed_data']['llm_analysis']
print(f"Extracted Data: {llm_analysis['extracted_data']}")
print(f"Model Used: {llm_analysis['model_info']['model']}")
print(f"Processing Cost: ${llm_analysis['model_info']['cost_estimate']:.4f}")

# Access document classification
classification = result['parsed_data']['document_classification']
print(f"Document Type: {classification['document_type']}")
print(f"Confidence: {classification['confidence']:.2f}")

# Convert to Markdown
markdown = convert_to_markdown(result)

# Save structured output
with open("output.json", "w") as f:
    json.dump(llm_analysis['extracted_data'], f, indent=2)
```

## ðŸ”’ Security & API Key Management

**NEVER commit API keys to version control!** od-parse provides secure configuration management:

### **Method 1: Environment File (Recommended)**

1. Copy the environment template:
```bash
cp .env.example .env
```

2. Edit `.env` with your actual API keys:
```bash
# .env file (never commit this!)
OPENAI_API_KEY=your-actual-openai-key-here
GOOGLE_API_KEY=your-actual-google-key-here
ANTHROPIC_API_KEY=your-actual-anthropic-key-here
```

### **Method 2: Secure Configuration in Code**

```python
from od_parse.config.env_config import get_api_key, setup_secure_environment

# Auto-loads from .env file
setup_secure_environment()

# Get API keys securely (no hardcoding!)
openai_key = get_api_key('openai')
google_key = get_api_key('google')

# Use in your code without exposing keys
result = parse_pdf("document.pdf")  # Uses configured keys automatically
```

### **Method 3: Environment Variables**

```bash
export OPENAI_API_KEY="your-api-key-here"
export GOOGLE_API_KEY="your-api-key-here"
export ANTHROPIC_API_KEY="your-api-key-here"
```

### **Security Best Practices**

- âœ… Use `.env` files (included in `.gitignore`)
- âœ… Use environment variables in production
- âœ… Use the secure configuration helper
- âŒ Never hardcode API keys in source code
- âŒ Never commit `.env` files to git
- âŒ Never share API keys in documentation

## LLM Provider Connectors

od-parse supports multiple LLM providers for document understanding:

### Supported Providers

| Provider | Models | Vision | Best For |
|----------|--------|--------|----------|
| **OpenAI** | GPT-4o, GPT-4o-mini, GPT-4-turbo | âœ… | General purpose, best accuracy |
| **Google** | Gemini 2.0 Flash, Gemini 1.5 Pro/Flash | âœ… | Large documents, cost-effective |
| **Anthropic** | Claude 3.5 Sonnet, Claude 3 Haiku | âœ… | Complex reasoning, long context |
| **Azure OpenAI** | GPT-4o (Azure hosted) | âœ… | Enterprise, compliance |
| **Hugging Face** | Sentence Transformers, local models | âœ… | Embeddings, privacy |
| **Ollama** | Llama 3, Mistral, Qwen (local) | âœ… | Offline, privacy, no API costs |

### Using Different Providers

```python
from od_parse import parse_pdf, get_available_models

# List all available models
print(get_available_models())
# ['gpt-4o', 'gpt-4o-mini', 'gemini-2.0-flash', 'claude-3-5-sonnet', ...]

# Use OpenAI
result = parse_pdf("document.pdf", llm_model="gpt-4o")

# Use Google Gemini (default)
result = parse_pdf("document.pdf", llm_model="gemini-2.0-flash")

# Use Anthropic Claude
result = parse_pdf("document.pdf", llm_model="claude-3-5-sonnet")

# Use local Ollama (no API key needed)
result = parse_pdf("document.pdf", llm_model="ollama-llama3")

# Pass API keys directly (instead of environment variables)
result = parse_pdf(
    "document.pdf",
    llm_model="gpt-4o",
    api_keys={"openai": "sk-your-key-here"}
)
```

### Environment Variables

```bash
# OpenAI
export OPENAI_API_KEY="sk-..."

# Google Gemini
export GOOGLE_API_KEY="..."

# Anthropic Claude
export ANTHROPIC_API_KEY="..."

# Azure OpenAI
export AZURE_OPENAI_API_KEY="..."
export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com"

# Hugging Face (for embeddings)
export HUGGINGFACE_API_KEY="..."
```

### Model Selection Guide

| Use Case | Recommended Model | Why |
|----------|-------------------|-----|
| Simple text extraction | `gemini-2.0-flash` | Fast, cheap, accurate |
| Complex forms/tables | `gpt-4o` or `claude-3-5-sonnet` | Best reasoning |
| Large documents (100+ pages) | `gemini-1.5-pro` | 1M token context |
| Budget-conscious | `gpt-4o-mini` or `claude-3-haiku` | Low cost |
| Privacy/offline | `ollama-llama3` | Runs locally |

## Advanced Features

od-parse includes cutting-edge AI features that can be enabled as needed:

### Smart Document Classification

Automatically identify document types with high accuracy using multi-signal analysis:

```python
from od_parse import parse_pdf
from od_parse.intelligence import DocumentType, DocumentClassifier

# Parse with smart classification enabled
result = parse_pdf("document.pdf", use_deep_learning=True)
classification = result['parsed_data']['document_classification']

# Get classification results
doc_type = classification['document_type']  # e.g., "tax_form_1040"
confidence = classification['confidence']   # e.g., 0.95
indicators = classification['key_indicators']  # e.g., {"ssn_found": "123-45-6789"}
suggestions = classification['suggestions']  # Processing recommendations

print(f"Document Type: {doc_type}")
print(f"Confidence: {confidence:.2f}")
print(f"Key Indicators: {indicators}")
```

**Supported Document Types (54+ types):**
- **Tax Documents**: Form 1040, W-2, 1099, Schedules A-D, etc.
- **Financial**: Bank statements, credit card statements, investment reports
- **Business**: Invoices, receipts, contracts, purchase orders, quotes
- **Legal**: Legal contracts, court documents, patents, wills
- **Medical**: Medical records, prescriptions, lab reports, insurance claims
- **Academic**: Research papers, transcripts, diplomas, certificates
- **Government**: Passports, driver's licenses, birth certificates
- **General**: Resumes, letters, reports, manuals, brochures

**Classification Features:**
- **Multi-Signal Analysis**: Text patterns, document structure, keywords, format detection, semantic analysis
- **High Accuracy**: 100% accuracy on tax forms, high confidence scoring
- **Key Extraction**: Automatic detection of SSNs, EINs, account numbers, etc.
- **Smart Suggestions**: Context-aware processing recommendations
- **Extensible**: Easy to add new document types and patterns

### Feature Configuration

```python
from od_parse.config import get_advanced_config

# Get configuration instance
config = get_advanced_config()

# Enable individual features
config.enable_feature('trocr')              # Transformer-based OCR
config.enable_feature('table_transformer')  # Neural table extraction
config.enable_feature('llava_next')         # Document understanding
config.enable_feature('multilingual')       # Multi-language support
config.enable_feature('quality_assessment') # Quality metrics
config.enable_feature('async_processing')   # Async processing

# Or use presets
config.enable_preset('advanced')  # Enable all stable features
```

### Direct Document Classification

Use the classifier independently for custom workflows:

```python
from od_parse.intelligence import DocumentClassifier, DocumentType

# Initialize classifier
classifier = DocumentClassifier()

# Classify parsed document data
parsed_data = {"text": "Form 1040 U.S. Individual Income Tax Return...",
               "tables": [], "forms": []}
analysis = classifier.classify_document(parsed_data)

# Access detailed analysis
print(f"Document Type: {analysis.document_type.value}")
print(f"Confidence: {analysis.confidence:.2f}")
print(f"Detected Patterns: {analysis.detected_patterns}")
print(f"Key Indicators: {analysis.key_indicators}")
print(f"Suggestions: {analysis.suggestions}")

# Check for specific document types
if analysis.document_type == DocumentType.TAX_FORM_1040:
    print("This is a tax form - extract tax-specific fields")
elif analysis.document_type == DocumentType.INVOICE:
    print("This is an invoice - extract billing information")
```

### TrOCR Text Recognition

Superior text recognition using transformer models:

```python
from od_parse.ocr import TrOCREngine

engine = TrOCREngine(model_name="microsoft/trocr-base-printed")
result = engine.extract_text("document.png")
print(f"Text: {result['text']}")
print(f"Confidence: {result['confidence']}")
```

### Neural Table Extraction

Advanced table detection and extraction:

```python
from od_parse.advanced import TableTransformerEngine

engine = TableTransformerEngine()
result = engine.extract_tables("document_with_tables.png")
for table in result['tables']:
    print(f"Table confidence: {table['confidence']}")
    print(f"Structure: {table['structure']}")
```

### Document Understanding with LLaVA-NeXT

AI-powered document understanding:

```python
from od_parse.advanced import LLaVANextEngine

engine = LLaVANextEngine()
result = engine.understand_document(
    "complex_document.png",
    "Analyze this document and describe its structure and content."
)
print(f"Understanding: {result['understanding']}")
```

### Multi-Language Support

Process documents in multiple languages:

```python
from od_parse.multilingual import MultilingualProcessor

processor = MultilingualProcessor()

# Detect language
detection = processor.detect_language("Este es un documento en espaÃ±ol.")
print(f"Language: {detection['language']}")

# Process with translation
result = processor.process_multilingual_text(
    "Bonjour, comment allez-vous?",
    target_language="en",
    include_translation=True
)
```

### Quality Assessment

Assess extraction quality and get recommendations:

```python
from od_parse.quality import assess_document_quality

quality = assess_document_quality(extraction_result)
print(f"Overall Score: {quality['overall_score']:.2f}")
for rec in quality['recommendations']:
    print(f"â€¢ {rec}")
```

### Async Processing

Process large batches asynchronously:

```python
import asyncio
from od_parse.async_processing import process_files_async

async def main():
    files = ["doc1.pdf", "doc2.pdf", "doc3.pdf"]
    results = await process_files_async(files, your_processor_function)

asyncio.run(main())
```

### Excel Processing

Process Excel files with DuckDB for fast SQL-based analysis:

```bash
# Install Excel dependencies
pip install od-parse[excel]
```

```python
from od_parse.excel import ExcelProcessor, excel_to_json, excel_to_markdown

# Quick conversion to JSON
json_data = excel_to_json("spreadsheet.xlsx")

# Quick conversion to Markdown
markdown = excel_to_markdown("spreadsheet.xlsx")

# Advanced processing with SQL queries
processor = ExcelProcessor("spreadsheet.xlsx")

# Load and get info
processor.load()
print(processor.get_sheet_names())
print(processor.get_column_info("Sheet1"))

# Run SQL queries on your Excel data
result = processor.query("SELECT * FROM Sheet1 WHERE amount > 1000")

# Export to JSON or Markdown
processor.to_json("output.json")
processor.to_markdown("output.md")
```

**Excel Processing Features:**
- âœ… Fast DuckDB-powered processing
- âœ… SQL queries on Excel data
- âœ… Multi-sheet support
- âœ… Automatic column type detection
- âœ… Skip unnamed/null columns automatically
- âœ… JSON and Markdown output formats

For detailed documentation on advanced features, see [ADVANCED_FEATURES.md](ADVANCED_FEATURES.md).

## ðŸ”’ Privacy & External APIs

**od-parse is privacy-first and works completely offline by default.** No data leaves your machine unless you explicitly configure external APIs.

### Default Behavior (100% Local)
- âœ… PDF parsing, OCR, table extraction - all local
- âœ… Advanced AI models (TrOCR, LLaVA-NeXT) - download once, run locally
- âœ… Quality assessment and multilingual detection - local processing
- âœ… No API keys required for core functionality

### Optional External APIs
External APIs are **only** used for optional enhancements:

| Feature | External Service | Required? | Local Alternative |
|---------|-----------------|-----------|-------------------|
| Translation | Google Translate | No | Language detection only |
| Cloud VLMs | OpenAI/Anthropic | No | Local LLaVA-NeXT model |
| Cloud OCR | Azure/AWS | No | Local TrOCR/Tesseract |

### Configuring External APIs (Optional)

**Preferred Method - Environment Variables:**
```bash
# Only set these if you want external API features
export GOOGLE_API_KEY="your-google-translate-key"        # For translation
export OPENAI_API_KEY="your-openai-key"                  # For cloud VLMs
export ANTHROPIC_API_KEY="your-anthropic-key"            # For Claude Vision
```

**Alternative - Configuration File:**
```yaml
# ~/.config/od-parse/config.yaml
api_keys:
  google: "your-google-translate-key"
  openai: "your-openai-key"
```

**Check Configuration:**
```python
from od_parse.config import get_advanced_config

config = get_advanced_config()
print(f"Google Translate: {config.has_api_key('google')}")
print(f"OpenAI: {config.has_api_key('openai')}")
```

> **ðŸ›¡ï¸ Security Note:** All external APIs are optional. The library works fully without any API keys.

## Advanced Usage

### Using the Unified Parser

```python
from od_parse.advanced.unified_parser import UnifiedPDFParser

# Initialize the parser with custom configuration
parser = UnifiedPDFParser({
    "use_deep_learning": True,
    "extract_handwritten": True,
    "extract_tables": True,
    "extract_forms": True,
    "extract_structure": True,
    "output_format": "json"
})

# Parse a PDF file
result = parser.parse("path/to/document.pdf")

# Convert to markdown
markdown = parser.to_markdown(result)

# Save markdown to file
with open("output.md", "w") as f:
    f.write(markdown)
```

### Using Different Configuration Profiles

```python
from od_parse.advanced.unified_parser import UnifiedPDFParser

# Example: Fast extraction (no deep learning)
fast_parser = UnifiedPDFParser({
    "use_deep_learning": False,
    "extract_tables": True,
    "extract_structure": False,
    "extract_handwritten": False
})
fast_result = fast_parser.parse("path/to/document.pdf")

# Example: Table extraction focus
table_parser = UnifiedPDFParser({
    "use_deep_learning": True,
    "extract_tables": True,
    "extract_forms": False,
    "extract_structure": False
})
table_result = table_parser.parse("path/to/document.pdf")

# Example: Form processing focus
form_parser = UnifiedPDFParser({
    "use_deep_learning": True,
    "extract_forms": True,
    "extract_tables": False
})
form_result = form_parser.parse("path/to/document.pdf")
```

### Enterprise Integrations

```python
from od_parse.advanced.unified_parser import UnifiedPDFParser
from od_parse.advanced.integrations import JSONFileConnector, CSVConnector, DatabaseConnector, VectorDBConnector

# Process a document with the unified parser
parser = UnifiedPDFParser()
result = parser.parse("path/to/document.pdf")

# Export to various formats
json_connector = JSONFileConnector({"file_path": "output.json"})
json_connector.export(result)

csv_connector = CSVConnector({"file_path": "tables.csv"})
csv_connector.export(result)

# Export to database
db_connector = DatabaseConnector({
    "db_type": "sqlite",
    "db_path": "documents.db"
})
db_connector.export(result)

# Export to vector database for RAG applications
vector_connector = VectorDBConnector({
    "db_type": "pgvector",
    "conn_string": "postgresql://user:password@localhost:5432/vectordb"
})
vector_connector.export(result)
```

### Configurable Vector Storage for RAG Applications

The `od-parse` library provides a flexible vector storage system that supports various embedding models and vector databases:

```python
from od_parse.advanced.unified_parser import UnifiedPDFParser
from od_parse.advanced.vector_storage import VectorStorage

def process_document(document_path, config=None):
    # Default configuration for the parser
    if config is None:
        config = {
            "use_deep_learning": True,
            "extract_tables": True,
            "extract_forms": True,
            "extract_structure": True
        }
    
    # Parse the document
    parser = UnifiedPDFParser(config)
    parsed_data = parser.parse(document_path)
    
    # Configure vector storage with your preferred embedding model and database
    vector_storage = VectorStorage({
        # Embedding model configuration
        "embedding_model": "openai",  # Options: openai, huggingface, cohere, custom
        "embedding_model_name": "text-embedding-3-small",
        "api_key": "your-api-key",  # Or use environment variable
        
        # Vector database configuration
        "vector_db": "pgvector",  # Options: pgvector, qdrant, pinecone, milvus, weaviate, chroma, json
        "connection_string": "postgresql://user:password@localhost:5432/vectordb",
        
        # Chunking configuration
        "chunk_size": 1000,
        "chunk_overlap": 100
    })
    
    # Create embeddings from parsed data
    embeddings = vector_storage.create_embeddings(parsed_data)
    
    # Store embeddings in the configured vector database
    vector_storage.store_embeddings(embeddings)
    
    return parsed_data
```

#### Using Custom Embedding Models

You can also use your own custom embedding models:

```python
# Create a custom embedding implementation
class MyEmbeddingModel:
    def __init__(self, model_path, **kwargs):
        # Load your model here
        self.model = load_my_model(model_path)
    
    def embed(self, text):
        # Generate embeddings using your model
        return self.model.generate_embedding(text)

# Save this to my_embeddings.py

# Then use it with VectorStorage
vector_storage = VectorStorage({
    "embedding_model": "custom",
    "custom_embedding_module": "/path/to/my_embeddings.py",
    "custom_embedding_class": "MyEmbeddingModel",
    "custom_embedding_params": {
        "model_path": "/path/to/my/model"
    }
})
```

### Using Vision Language Models (VLMs)

The library now supports Vision Language Models like Qwen 2.5 VL to enhance document understanding by leveraging both visual and textual information:

```python
from od_parse.advanced.unified_parser import UnifiedPDFParser
from od_parse.advanced.vlm_processor import VLMProcessor

# Initialize the VLM processor with Qwen 2.5 VL
vlm_processor = VLMProcessor({
    "model": "qwen2.5-vl",  # Options: qwen2.5-vl, claude-3-opus-vision, gemini-pro-vision
    "api_key": "your-api-key",  # Or use environment variable
    "max_tokens": 2048,
    "temperature": 0.2,
    "system_prompt": "You are an expert document analyzer. Extract all information from this document."
})

# Parse a document with traditional methods first
parser = UnifiedPDFParser({"extract_tables": True, "extract_forms": True})
parsed_data = parser.parse("path/to/document.pdf")

# Convert the PDF to images for VLM processing
from pdf2image import convert_from_path
images = convert_from_path("path/to/document.pdf")
first_page_image = images[0]
first_page_image.save("first_page.png")

# Enhance parsing results with VLM analysis
enhanced_data = vlm_processor.enhance_parsing_results(parsed_data, "first_page.png")

# Extract tables specifically using VLM
tables = vlm_processor.extract_tables_with_vlm("first_page.png")

# Extract form fields using VLM
form_fields = vlm_processor.extract_form_fields_with_vlm("first_page.png")

# Process a document image directly with a custom prompt
vlm_analysis = vlm_processor.process_document_image(
    "first_page.png",
    "Analyze this document and identify any handwritten notes, signatures, or annotations."
)
print(vlm_analysis["analysis"])
```

#### Benefits of Using VLMs

- **Superior handling of complex layouts**: VLMs can understand document structure visually
- **Better extraction of handwritten content**: VLMs excel at recognizing handwriting
- **Improved table extraction**: VLMs can understand tables even with complex formatting
- **Form field detection**: VLMs can identify form fields and their values more accurately
- **Context-aware understanding**: VLMs consider both the visual layout and textual content

## Requirements

- Python 3.8+

### Core Dependencies (Installed by Default)
These are lightweight and installed automatically:
- pdfminer.six, PyMuPDF - PDF text extraction
- Pillow, pdf2image - Image handling
- opencv-python - Image processing
- tabula-py, pdfplumber - Table extraction
- numpy, pandas - Data processing
- openai, anthropic, google-generativeai - LLM APIs

### Excel Processing (Optional)
```bash
pip install od-parse[excel]
```
- duckdb - Fast SQL-based Excel processing
- openpyxl - Excel file reading

### Advanced AI Features (Optional - Heavy ~2GB+)
```bash
pip install od-parse[advanced]
```
These require PyTorch and are only needed for cutting-edge ML features:
- torch, torchvision - PyTorch framework
- transformers - Hugging Face models
- TrOCR - When Tesseract OCR isn't accurate enough
- Table Transformer - For complex table layouts
- LLaVA-NeXT - For visual document understanding

> **ðŸ’¡ Note:** 95% of use cases work perfectly with core dependencies. Only install advanced features if you specifically need transformer-based OCR or neural table extraction.

## Configuration System

The `od-parse` library now uses a flexible configuration system that allows you to customize all settings, including API endpoints, model names, and system prompts, without modifying the code:

### Configuration Files

You can create a configuration file in YAML or JSON format. The library will look for configuration files in the following locations:

1. Path specified when calling `load_config()`
2. `~/.config/od-parse/config.yaml` or `~/.config/od-parse/config.json`
3. `./od-parse.yaml` or `./od-parse.json` in the current working directory

Example configuration file (YAML):

```yaml
# API keys (IMPORTANT: For security, use environment variables instead of storing keys here)
# Uncomment and replace with your actual keys if needed
api_keys:
  # openai: "your-openai-api-key"  # Or set OPENAI_API_KEY environment variable
  # anthropic: "your-anthropic-api-key"  # Or set ANTHROPIC_API_KEY environment variable
  # cohere: "your-cohere-api-key"  # Or set COHERE_API_KEY environment variable
  # huggingface: "your-huggingface-api-key"  # Or set HUGGINGFACE_API_KEY environment variable
  # google: "your-google-api-key"  # Or set GOOGLE_API_KEY environment variable

# API endpoints
api_endpoints:
  openai: https://api.openai.com/v1
  anthropic: https://api.anthropic.com
  cohere: https://api.cohere.ai/v1

# VLM models
vlm_models:
  qwen: qwen2.5-vl
  claude: claude-3-opus-vision
  gemini: gemini-pro-vision

# Embedding models
embedding_models:
  openai: text-embedding-3-small
  cohere: embed-english-v3.0
  huggingface: sentence-transformers/all-mpnet-base-v2

# System prompts
system_prompts:
  document_analysis: >
    You are an expert document analyzer. Analyze the document image and extract all relevant information.
```

### API Keys Configuration

There are three ways to configure API keys for embedding models, VLMs, and other services:

1. **Using environment variables (recommended for security)**:

   ```bash
   # Standard API keys (recommended)
   export OPENAI_API_KEY="your-openai-api-key"
   export ANTHROPIC_API_KEY="your-anthropic-api-key"
   export COHERE_API_KEY="your-cohere-api-key"
   export HUGGINGFACE_API_KEY="your-huggingface-api-key"
   export GOOGLE_API_KEY="your-google-api-key"
   
   # Library-specific API keys (alternative)
   export OD_PARSE_OPENAI_API_KEY="your-openai-api-key"
   export OD_PARSE_ANTHROPIC_API_KEY="your-anthropic-api-key"
   ```

2. **In the configuration file** (less secure, but convenient for development):

   ```yaml
   # In od-parse.yaml or ~/.config/od-parse/config.yaml
   api_keys:
     openai: "your-openai-api-key"
     anthropic: "your-anthropic-api-key"
     cohere: "your-cohere-api-key"
   ```

3. **Directly in code** when initializing components:

   ```python
   vector_storage = VectorStorage({
       "embedding_model": "openai",
       "api_key": "your-openai-api-key"  # Takes precedence over config files and env vars
   })
   ```

### Other Environment Variables

You can also configure other settings using environment variables:

```bash
# API endpoints
export OD_PARSE_OPENAI_API_URL="https://api.openai.com/v1"
export OD_PARSE_ANTHROPIC_API_URL="https://api.anthropic.com"

# VLM models
export OD_PARSE_QWEN_MODEL="qwen2.5-vl"
export OD_PARSE_CLAUDE_MODEL="claude-3-opus-vision"

# Embedding models
export OD_PARSE_OPENAI_EMBEDDING_MODEL="text-embedding-3-small"
```

### Using the Configuration System in Code

```python
from od_parse.config.settings import load_config, get_config

# Load configuration from file
load_config("/path/to/config.yaml")

# Get configuration values
api_endpoint = get_config("api_endpoints.openai")
qwen_model = get_config("vlm_models.qwen")

# Get nested configuration with default value
chroma_path = get_config("vector_db.chroma.default_path", "./default_chroma_db")
```

## Command Line Interface

The library includes a command-line interface for quick PDF processing:

```bash
# Basic usage
python -m od_parse.main document.pdf --output-file output.json

# Advanced usage with pipeline selection
python -m od_parse.main document.pdf --pipeline full --output-format markdown --output-file output.md

# Fast processing (optimized for speed)
python -m od_parse.main document.pdf --pipeline fast --output-format summary

# Extract only tables
python -m od_parse.main document.pdf --pipeline tables --output-format json

# Use deep learning capabilities (includes smart document classification)
python -m od_parse.main document.pdf --deep-learning
```

## Enterprise Applications

This library is designed for enterprise AI applications:

- **Document Classification & Routing**: Automatically classify and route documents based on type (tax forms, invoices, contracts, etc.)
- **RAG Systems**: Extract and embed document content for retrieval-augmented generation
- **AI Agents**: Provide structured data and document intelligence for AI agents to work with
- **Document Automation**: Automate document processing workflows with intelligent classification
- **Data Unification**: Extract structured data from unstructured documents with type-aware processing
- **Knowledge Bases**: Build searchable knowledge bases from document repositories with smart categorization
- **Compliance & Audit**: Automatically detect sensitive documents (tax forms, medical records) for compliance workflows

## Development

### Running Tests Locally

```bash
# Activate virtual environment
source venv/bin/activate

# Run all tests (130+ tests)
python -m pytest tests/ -v

# Run with coverage
python -m pytest tests/ -v --cov=od_parse --cov-report=term-missing

# Run specific test file
python -m pytest tests/test_parser.py -v

# Run specific test
python -m pytest tests/test_parser.py::TestPDFParser::test_extract_text_with_sample_pdf -v
```

### Test Coverage

| Category | Tests | Status |
|----------|-------|--------|
| Core parsing | 40+ | âœ… All pass |
| Excel processing | 10+ | âœ… All pass |
| LLM integration | 20+ | âœ… All pass |
| Advanced features | 50+ | âœ… All pass |
| PyTorch features | 3 | â­ï¸ Skipped (optional deps) |

> **Note:** 3 tests require PyTorch (~2GB) and are skipped by default. These test TrOCR, Table Transformer, and LLaVA-NeXT features.

### Pre-Commit Checks

The `pre_commit_check.sh` script runs all the same checks as CI:

- âœ… **Black formatting check** - Ensures code is properly formatted
- âœ… **Flake8 linting** - Checks for code quality issues
- âœ… **Type checking (mypy)** - Optional type checking
- âœ… **Security scan (bandit)** - Security vulnerability scanning
- âœ… **Tests** - Runs the full test suite with coverage

### Manual Checks

You can also run individual checks:

```bash
# Format code with black
black od_parse examples

# Check formatting without changing files
black --check od_parse examples

# Run linting
flake8 od_parse

# Run type checking
mypy od_parse --ignore-missing-imports --no-strict-optional

# Run security scan
bandit -r od_parse -ll
```

## License

MIT License
